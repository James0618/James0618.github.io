<!-- 博文的布局-Layout -->
<!DOCTYPE html>
<html>
<head>
<!-- 引入head标签 -->
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-sclable=0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="description" content="这是一个初学博客的博主" />
<meta name="keywords" content="Robot, Artificial Intelligent, Diary" />
<link rel="stylesheet" href="/assets/css/style.css">
<link rel="stylesheet" href="/assets/css/media.css">
<link rel="stylesheet" href="/assets/css/animate.min.css">
<link rel="stylesheet" href="/assets/css/pygments/pygments_default.css">
<link rel="stylesheet" href="/assets/css/github-markdown.css">
<!-- SNS-icon -->
<script src="//at.alicdn.com/t/font_856428_y9z6nq7zf5.js"></script>
<!-- share.css -->
<link rel="stylesheet" href="/assets/css/share.min.css">
<!-- font -->
<link rel="stylesheet" href="/assets/css/font.css">
<link rel="dns-prefetch" href="//cdn.bootcss.com" />
<!-- <link href="https://fonts.googleapis.com/css?family=Kaushan+Script|Pacifico|Ubuntu|Roboto+Mono|Source+Sans+Pro" rel="stylesheet"> -->

<!-- Favicon -->
<link href="/assets/profile.jpeg" rel="shortcut icon" />
<link href="/assets/profile.jpeg" rel="apple-touch-icon-precomposed" />
<!-- Android Lolipop Theme Color -->
<!-- <meta name="theme-color" content="#1464FB"> -->
<title>PyTorch 入门(2)</title>
<!-- 百度统计 -->

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

<!-- 谷歌分析 -->

<script type="text/x-mathjax-config">
var mathId = document.getElementById("post-content"); //选择公式识别范围
MathJax.Hub.Config({
    showProcessingMessages: false, //关闭js加载过程信息
    messageStyle: "none", //不显示信息
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
        inlineMath:  [ ["$", "$"] ], //行内公式选择$
        displayMath: [ ["$$","$$"] ], //段内公式选择$$
    },
    "HTML-CSS": {
        availableFonts: ["STIX","TeX"], //可选字体
        showMathMenu: true //关闭右击菜单显示
    }
});
MathJax.Hub.Queue(["Typeset",MathJax.Hub,mathId]);
</script>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<!-- Android Lolipop Theme Color -->
<meta name="theme-color" content=" rgb(255,210,32) ">
</head>
<body>

<!-- 顶部锚点 -->
<a id="htmlup" name="htmlup"></a>
<!-- 引入博文顶部选项 -->

<header id="post-header" style="background-color:rgb(255,210,32);">
  <div class="top-center">
      <div class="logo">
          <a href="/" title="my awesome webtitle" style="color: white;">James' HomePage</a>
      </div>
      <nav class="top-nav">
          <ul>
              
                <li><a href="/" style="color: white;">首页</a></li>
              
                <li><a href="/tags.html" style="color: white;">标签</a></li>
              
                <li><a href="/timeline.html" style="color: white;">时间线</a></li>
              
                <li><a href="/about.html" style="color: white;">关于博主</a></li>
              
                <li><a href="/friendLink.html" style="color: white;">友情链接</a></li>
              
          </ul>
      </nav>
      <div id="top-boot">
        <a href="javascript:;" id="boot1" style="display:block;" onclick="document.getElementById('boot-area').style.display='block';document.getElementById('boot1').style.display='none';document.getElementById('boot2').style.display='block';"><img src="/assets/boot_white.png" alt=""></a>
        <a href="javascript:;" id="boot2" style="display: none;" onclick="document.getElementById('boot-area').style.display='none';document.getElementById('boot1').style.display='block';document.getElementById('boot2').style.display='none';"><img src="/assets/boot_white.png" alt=""></a>
      </div>
  </div>

</header>


<!-- 引入移动下拉选项 -->
<div id="boot-area">
    <ul>
        
          <a href="/"><li>首页</li></a>
        
          <a href="/tags.html"><li>标签</li></a>
        
          <a href="/timeline.html"><li>时间线</li></a>
        
          <a href="/about.html"><li>关于博主</li></a>
        
          <a href="/friendLink.html"><li>友情链接</li></a>
        
    </ul>
</div>

<!-- 引入博文顶部样式 -->
<!-- 版本一 垃圾 -->
<!-- <div class="wow fadeIn top" data-wow-duration="3.5s" >
    <span class="wow fadeInUp" data-wow-delay="0.2s">PyTorch 入门(2)</span>
    <span class="wow fadeInUp" data-wow-delay="0.4s"></span>
    <span class="wow fadeInUp" data-wow-delay="0.4s"></span>
    <span class="wow fadeInUp" data-wow-delay="0.6s">作者&nbsp;&nbsp;|&nbsp;&nbsp;James</span>
</div> -->

<!-- 版本二 可切换页面 -->

<div class="post-top" style="background-color:rgb(255,210,32);">
  <!-- 页面宽度大于800px -->
  <div class="left-area">
    
      <a href="javascript:;" class="btn bounceInLeft animated" onmouseover="showLeft();this.style.color='rgb(255,210,32)';" onmouseout="goneLeft();this.style.color='rgba(0,0,0,.2)';"><</a>
      <div id="left-tab" style="display:none;"><span class="left-san"></span><span class="left-main" style="color:rgb(255,210,32);"><sapn class="main">没有上一页咯</sapn></span></div>
    
  </div>
  <div class="post-titlearea">
    <span class="wow fadeInUp" data-wow-delay="0.2s">PyTorch 入门(2)</span>
    <!-- <span class="wow fadeInUp" data-wow-delay="0.4s"></span> -->
    <!-- <span class="wow fadeInUp" data-wow-delay="0.4s"></span> -->
    <!-- <span class="wow fadeInUp" data-wow-delay="0.6s">作者&nbsp;&nbsp;|&nbsp;&nbsp;James</span> -->
  </div>
  <div class="right-area">
    
      <a href="/2019/08/02/PyTorch%E5%85%A5%E9%97%A8(1).html" class="btn bounceInRight self-animated" onmouseover="showRight();this.style.color='rgb(255,210,32)';" onmouseout="goneRight();this.style.color='rgba(0,0,0,.2)';">></a>
      <div id="right-tab" style="display:none;"><span class="right-san"></span><span class="right-main" style="color:rgb(255,210,32);"><sapn class="main">PyTorch 入门(1)</sapn></span></div>
    
  </div>

  <!-- 页面宽度小于800px -->
  <div class="post-changearea">
    
      <a href="javascript:;" class="leftchange" style="border-right: 1px solid rgb(255,210,32);border-bottom: 2px solid rgb(255,210,32);"><span><br>没有上一篇咯</span></a>
    
    
      <a href="/2019/08/02/PyTorch%E5%85%A5%E9%97%A8(1).html" class="rightchange" style="border-left: 1px solid rgb(255,210,32);border-bottom: 2px solid rgb(255,210,32);"><span>下一篇<br><br>PyTorch 入门(1)</span></a>
    
  </div>
</div>


<div class="markdown-body fadeInUp animated">

  

  <!-- 文章内容 -->
  <h1 id="pytorch入门2">PyTorch入门(2)</h1>

<h2 id="分类任务">分类任务</h2>

<p>​		在分类任务中，需要对数据生成类别，我们通常不使用一个数字来表示类别比如1, 2, 3, 4, 5等，而是使用不同的神经元来表示类别，最终的输出类似于<code class="highlighter-rouge">0 0 1</code>、<code class="highlighter-rouge">0 1 0</code>等，而用到的$Loss Function$也多使用<code class="highlighter-rouge">CrossEntropyLoss()</code>，也就是交叉熵，最后需要使用<code class="highlighter-rouge">torch.max(net(x), 1)[1]</code>选出其中概率最大的作为选择的类别进行输出，其有两个维度<code class="highlighter-rouge">torch.max(net(x), 1)[0]</code>则是最大的数值，那么<code class="highlighter-rouge">[1]</code>就是对应的神经元了。</p>

<p>​		其他的部分和之前的都是一样的，在创建数据的时候使用了一个函数<code class="highlighter-rouge">torch.cat</code>，这个函数把不同类别的数据放到了一起然后转换成想要的数据类型：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="nb">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">y0</span><span class="p">,</span> <span class="n">y1</span><span class="p">),</span> <span class="p">)</span><span class="o">.</span><span class="nb">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">)</span>
</code></pre></div></div>

<p>​		其参数dim是指拼接哪一个维度的数据，默认为0，上面的程序就将数据的第0维进行拼接，从两个[100, 2]的张量变成了一个[200, 2]的张量。</p>

<p>​		接下来在莫烦的教程中还有一个没有用过的函数<code class="highlighter-rouge">plt.scatter()</code>，之前使用matplotlib绘图都是直接plot大法好，这个高级的东西还是第一次见。它的用法介绍如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">matplotlib</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">verts</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">hold</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: center">参数</th>
      <th style="text-align: center">格式</th>
      <th style="text-align: center">作用</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">x,y</td>
      <td style="text-align: center">形如shape(n, )数组</td>
      <td style="text-align: center">输入数据</td>
    </tr>
    <tr>
      <td style="text-align: center">s</td>
      <td style="text-align: center">标量或形如shape(n, )数组，默认：20</td>
      <td style="text-align: center">size in points^2</td>
    </tr>
    <tr>
      <td style="text-align: center">c</td>
      <td style="text-align: center">色彩或颜色序列，可选</td>
      <td style="text-align: center">C可以是一个RGB或者RGBA的二维行数组</td>
    </tr>
    <tr>
      <td style="text-align: center">marker</td>
      <td style="text-align: center">MarkerStyle，可选</td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: center">cmap</td>
      <td style="text-align: center">Colormap可选，默认None</td>
      <td style="text-align: center"> </td>
    </tr>
  </tbody>
</table>

<p>​		具体可以参照https://blog.csdn.net/qiu931110/article/details/68130199中介绍的属性。在教程中执行了以下命令：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">pred_y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'RdYlGn'</span><span class="p">)</span>
</code></pre></div></div>

<p>​		这其中，前两个是需要画出来的数据两个维度的值，<code class="highlighter-rouge">c</code>则使用了一个小技巧，直接使用数据的标签当做色彩来对不同类别的数据进行区分，而不用手动判断类别后再进行绘制，而<code class="highlighter-rouge">lw</code>则是指线的宽度$linewidths$。</p>

<h2 id="快速搭建法">快速搭建法</h2>

<p>​		可以使用<code class="highlighter-rouge">torch.nn.Sequential()</code>直接替代先前的网络搭建部分，具体如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div>

<p>​		可以看到，激活函数在这里也被当做一层网络写在了函数里。</p>

<h2 id="保存--恢复网络">保存 &amp; 恢复网络</h2>

<p>​		有两种方式来进行网络的保存和提取，一种是保存整个网络，另一种是只保存网络中的参数：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="s">'net.pkl'</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s">'net_params.pkl'</span><span class="p">)</span>
</code></pre></div></div>

<p>​		前者占用的空间更大速度也更慢，区别体现在恢复网络的时候。保存整个网络的方法在恢复网络的时候无需新建一个网络，只需要：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'net.pkl'</span><span class="p">)</span>
</code></pre></div></div>

<p>​		而只保存参数的方法则需要重建一个相同的网络再将参数放置到新建的网络中：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">net</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'net_params.pkl'</span><span class="p">))</span>
</code></pre></div></div>

<h2 id="批训练">批训练</h2>

<p>​		在PyTorch中可以使用<code class="highlighter-rouge">DataLoader</code>进行简单的数据包装，经过包装后数据可以高效的迭代以进行训练，这一工具可以将自己的numpy array的数据转换成Tensor：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 先转换成 torch 能识别的 Dataset
</span><span class="n">torch_dataset</span> <span class="o">=</span> <span class="n">Data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">(</span><span class="n">data_tensor</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">target_tensor</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># 把 dataset 放入 DataLoader
</span><span class="n">loader</span> <span class="o">=</span> <span class="n">Data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">torch_dataset</span><span class="p">,</span>      <span class="c1"># torch TensorDataset format
</span>    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>      <span class="c1"># mini batch size
</span>    <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>               <span class="c1"># 要不要打乱数据 (打乱比较好)
</span>    <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>              <span class="c1"># 多线程来读数据
</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">loader</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'Epoch: '</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="s">'| Step: '</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="s">'| batch x: '</span><span class="p">,</span>
              <span class="n">batch_x</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s">'| batch y: '</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</code></pre></div></div>

<p>​		之后运行的结果如下所示：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
Epoch:0 | Step:0 | batch x:[6. 7. 2. 3. 1.] | batch y:[5. 4. 9. 8. 10.]
Epoch:0 | Step:1 | batch x:[9. 10. 4. 8. 5.] | batch y:[2. 1. 7. 3. 6.]
Epoch:1 | Step:0 | batch x:[3. 4. 2. 9. 10.] | batch y:[8. 7. 9. 2. 1.]
Epoch:1 | Step:1 | batch x:[1. 7. 8. 5. 6.] | batch y:[10. 4. 3. 6. 5.]
Epoch:2 | Step:0 | batch x:[3. 9. 2. 6. 7.] | batch y:[8. 2. 9. 5. 4.]
Epoch:2 | Step:1 | batch x:[10. 4. 8. 1. 5.] | batch y:[1. 7. 3. 10. 6.]
"""</span>
</code></pre></div></div>

<h2 id="关于variable-和-tensor">关于Variable 和 Tensor</h2>

<p>​		在学习莫烦的教程中关于优化器的部分时，我注意到了在新版本PyTorch中有关Variable和Tensor之间关系的改变，不过在官方文档中没有发现什么。</p>

<p>​		在教程的视频中对从TensorDataset里获取到的batch数据进行了转换为Variable的操作，解释道是因为只有Variable才能进行自动求导，官方文档也是这么说的。但事实上在教程的代码中取消了这一操作，而在Pycharm的Python控制台中查看变量的类型时，可以清晰地看到所有数据都是Tensor类型的。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># training
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCH</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Epoch: '</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">b_x</span><span class="p">,</span> <span class="n">b_y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">loader</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">net</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">l_his</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">nets</span><span class="p">,</span> <span class="n">optimizers</span><span class="p">,</span> <span class="n">losses_his</span><span class="p">):</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">b_x</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">b_y</span><span class="p">)</span>
            <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">l_his</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</code></pre></div></div>

<p>​		询问了同学后得知Tensor变量只需要它的<code class="highlighter-rouge">require_grad</code>属性是<code class="highlighter-rouge">True</code>的就可以进行自动求导，经过查证果然如此！上述代码中的b_x和b_y的<code class="highlighter-rouge">require_grad</code>属性是<code class="highlighter-rouge">False</code>的，但是loss的<code class="highlighter-rouge">require_grad</code>属性是<code class="highlighter-rouge">True</code>的，这应该是在执行<code class="highlighter-rouge">loss_func</code>时为其加上的属性，这样便能够进行<code class="highlighter-rouge">backward()</code>操作了！</p>

<p>​		（所以事实证明Python Console是个好东西）</p>

  <!-- 引入share模块 -->
  
  <div class="social-share-wrapper">
    <div class="social-share"></div>
  </div>


<!-- share.js -->
<script src="/assets/js/social-share.min.js"></script>
<script>
  socialShare('.social-share', {
    sites: [
      
        'qq'
        ,
        
      
        'wechat'
        ,
        
      
        'weibo'
        ,
        
      
        'twitter'
        ,
        
      
        'facebook'
        
      
    ],
    wechatQrcodeTitle: "分享到微信朋友圈",
    wechatQrcodeHelper: '期待在朋友圈见到这篇文章'
  });
</script>

</div>

<!-- 底部锚点 -->
<a id="htmldown" name="htmldown"></a>
<!-- 引入评论模块 -->



    <section class="post-footer-item comment">
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zNDI2OS8xMDgwNg=="></div>
    </section>

    <!-- 来必力City版安装代码 -->
    <script type="text/javascript">
       (function(d, s) {
           var j, e = d.getElementsByTagName(s)[0];

           if (typeof LivereTower === 'function') { return; }

           j = d.createElement(s);
           j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
           j.async = true;

           e.parentNode.insertBefore(j, e);
       })(document, 'script');
    </script>
    <noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
    <!-- City版安装代码已完成 -->





<!-- 引入goto模块 -->
<div class="bounceInRight animated go">
  <a title="顶部切换页面" class="gototop" href="#htmlup" target="_self">
    <div class="box" style="font-family:'ffad_matroregular';">
        Top
    </div>
  </a>
  <a title="底部有livere评论哦" class="gotobottom" href="#htmldown" target="_self">
    <div class="box" style="font-family:'ffad_matroregular';">
        Foot
    </div>
  </a>
</div>

<!-- 引入页面底部模块 -->
<footer id="bottom">
  <br>
  <span>James' HomePage ©
  
  
    2018
    -
  
  2019
  <br>
  Powered by <a href="https://www.jekyll.com.cn/">Jekyll</a> | <a href="https://github.com/xukimseven/HardCandy-Jekyll">HardCandy-Jekyll</a></span>
</footer>


<!-- 引用wow.js的动画效果 -->
<script src="/assets/js/wow.js"></script>
<script>
    var wow = new WOW({
        boxClass: 'wow',
        animateClass: 'animated',
        // offset: 600,
        mobile: true,
        live: true
    });
    wow.init();
</script>
<!-- 页面刷新回到顶部 -->
<script>
    window.onbeforeunload = function(){
        //刷新后页面自动回到顶部
        document.documentElement.scrollTop = 0;  //ie下
        document.body.scrollTop = 0;  //非ie
    }
</script>
<script src="/assets/js/main.js"></script>
</body>
</html>
