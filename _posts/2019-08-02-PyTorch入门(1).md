---
layout: post
title: 'PyTorch 入门(1)'
date: 2019-08-02
author: James
color: rgb(255,210,32)
cover: '../2019/08/02/cover.jpg'
tags: PyTorch
---

# PyTorch

## Pytorch 中的求导

​		在Pytorch中的`backward()`是用于计算图的求导的，首先变量有一个$requires\_grad$参数，如果设置为$True$则需要进行求导，在求导过程中：

```python
out.backward()
```

​		是对其最后方设置了`requires_grad=True`的变量进行求导，例如下面这段程序，我们将从它及其它的变体来理解`backward()`：

```python
import torch
from torch.autograd import Variable


tensor = torch.FloatTensor([2])
variable = Variable(tensor, requires_grad=True)
test = Variable(tensor)

v_1 = 3 * variable
v_2 = 4 * v_1

v_2.backward()
print("variable.grad: \n", variable.grad)
```

​		上面这段程序是很好理解的，由于variable是最后方设置了`requires_grad=True`的参数，因此在执行`v_2.backward()`的时候就相当于对`variable`求导，最终的结果如下所示：

```
variable.grad: 
 tensor([12.])
```

​		结果自然就是$3\times 4=12$了，但需要说明的是`backward()`方法不支持对非标量进行求导，因此提供了一个$gradient$参数用骚操作实现对非标量求导。在执行`out.backward(gradient)`的时候，如果out不是一个标量，那么先构造一个标量的值：`L = torch.sum(z*gradient)`，再计算关于L对各个叶子变量的梯度。因此`backward()`在没有任何参数时相当于执行了`out.backward(gradient=torch.FloatTensor([1]))`，也就是在结果上乘了一个1再求导，这也解释了为什么在`gradient=torch.FloatTensor([2])`的时候结果会乘2。

​		最后说明一下，`backward()`的求导是真真切切的对变量求导，变量的平方的求导和变量的拷贝乘平方的结果是不一样的！



## 建立神经网络

+ `unsqueeze(torch.linspace(-1, 1, 100), dim=1)`：由于torch只能处理二维的数据，因此需要用这个操作增加一个维度（从0开始），比如这个语句就实现了从`torch.Size([100])`到`torch.Size([100, 1])`的转换。
+ `class Net(torch.nn.Module):`：在定义网络的时候需要继承torch中神经网络的模型`torch.nn.Module`。
+ `forward()`：每一个网络中都会有这个方法

​    在此叙述一下使用torch搭建网络所需要环节和步骤。首先是一个描述网络的类，其元素和方式在上面已经列出，其中在初始化函数使用`torch.nn`的各种层来对网络进行描述，在`forward()`中对网络的结构进行描述：

```python
class Net(torch.nn.Module):
    def __init__(self, n_feature, n_hidden, n_output):
        super(Net, self).__init__()
        self.hidden = torch.nn.Linear(n_feature, n_hidden)
        self.predict = torch.nn.Linear(n_hidden, n_output)

    def forward(self, x):
        hidden_out = F.relu(self.hidden(x))
        result = self.predict(hidden_out)
        return result
```

​		在完成了对网络的描述后需要进行训练或者说学习，在学习时需要构建损失函数和优化器，在构建优化器的时候需要将网络的参数传进去并设置学习率：

```python
optimizer = torch.optim.SGD(net.parameters(), lr=0.2)
loss_func = torch.nn.MSELoss()
```

​		至于训练，则需要使用网络计算对应x的输出`prediction`，之后使用预设的损失函数计算`loss`，接下来首先将优化器的梯度置为0，这是因为每次优化结束梯度仍然会保持上一次的结果，然后对loss这个Variable变量进行`backward()`操作，最后执行优化器的梯度下降操作：

```python
for t in range(1000):
    prediction = net(x)
    loss = loss_func(prediction, y)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

​		`optimizer.zero_grad()`这一步至关重要！一定不要忘记！


