---
layout: post
title: 'PyTorch 入门(2)'
date: 2019-08-05
author: James
color: rgb(255,210,32)
cover: '../2019/08/02/cover.jpg'
tags: PyTorch
---

# PyTorch入门(2)

## 分类任务

​		在分类任务中，需要对数据生成类别，我们通常不使用一个数字来表示类别比如1, 2, 3, 4, 5等，而是使用不同的神经元来表示类别，最终的输出类似于`0 0 1`、`0 1 0`等，而用到的$Loss Function$也多使用`CrossEntropyLoss()`，也就是交叉熵，最后需要使用`torch.max(net(x), 1)[1]`选出其中概率最大的作为选择的类别进行输出，其有两个维度`torch.max(net(x), 1)[0]`则是最大的数值，那么`[1]`就是对应的神经元了。

​		其他的部分和之前的都是一样的，在创建数据的时候使用了一个函数`torch.cat`，这个函数把不同类别的数据放到了一起然后转换成想要的数据类型：

```python
n_data = torch.ones(100, 2)
x0 = torch.normal(2 * n_data, 1)
y0 = torch.zeros(100)
x1 = torch.normal(-2 * n_data, 1)
y1 = torch.ones(100)

x = torch.cat((x0, x1), 0).type(torch.FloatTensor)
y = torch.cat((y0, y1), ).type(torch.LongTensor)
```

​		其参数dim是指拼接哪一个维度的数据，默认为0，上面的程序就将数据的第0维进行拼接，从两个[100, 2]的张量变成了一个[200, 2]的张量。

​		接下来在莫烦的教程中还有一个没有用过的函数`plt.scatter()`，之前使用matplotlib绘图都是直接plot大法好，这个高级的东西还是第一次见。它的用法介绍如下：

```python
matplotlib.pyplot.scatter(x, y, s=20, c='b', marker='o', cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, verts=None, hold=None, **kwargs)
```

|  参数  |                格式                |                作用                |
| :----: | :--------------------------------: | :--------------------------------: |
|  x,y   |         形如shape(n, )数组         |              输入数据              |
|   s    | 标量或形如shape(n, )数组，默认：20 |          size in points^2          |
|   c    |        色彩或颜色序列，可选        | C可以是一个RGB或者RGBA的二维行数组 |
| marker |         MarkerStyle，可选          |                                    |
|  cmap  |       Colormap可选，默认None       |                                    |

​		具体可以参照https://blog.csdn.net/qiu931110/article/details/68130199中介绍的属性。在教程中执行了以下命令：

```python
plt.scatter(x.numpy()[:, 0], x.numpy()[:, 1], c=pred_y, s=100, lw=0, cmap='RdYlGn')
```

​		这其中，前两个是需要画出来的数据两个维度的值，`c`则使用了一个小技巧，直接使用数据的标签当做色彩来对不同类别的数据进行区分，而不用手动判断类别后再进行绘制，而`lw`则是指线的宽度$linewidths$。



## 快速搭建法

​		可以使用`torch.nn.Sequential()`直接替代先前的网络搭建部分，具体如下：

```python
net2 = torch.nn.Sequential(
    torch.nn.Linear(1, 10),
    torch.nn.ReLU(),
    torch.nn.Linear(10, 1)
)
```

​		可以看到，激活函数在这里也被当做一层网络写在了函数里。



## 保存 & 恢复网络

​		有两种方式来进行网络的保存和提取，一种是保存整个网络，另一种是只保存网络中的参数：

```python
torch.save(net, 'net.pkl')
torch.save(net.state_dict(), 'net_params.pkl')
```

​		前者占用的空间更大速度也更慢，区别体现在恢复网络的时候。保存整个网络的方法在恢复网络的时候无需新建一个网络，只需要：

```python
net = torch.load('net.pkl')
```

​		而只保存参数的方法则需要重建一个相同的网络再将参数放置到新建的网络中：

```python
net = torch.nn.Sequential(
    torch.nn.Linear(1, 10),
    torch.nn.ReLU(),
    torch.nn.Linear(10, 1)
)

net.load_state_dict(torch.load('net_params.pkl'))
```



## 批训练

​		在PyTorch中可以使用`DataLoader`进行简单的数据包装，经过包装后数据可以高效的迭代以进行训练，这一工具可以将自己的numpy array的数据转换成Tensor：

```python
# 先转换成 torch 能识别的 Dataset
torch_dataset = Data.TensorDataset(data_tensor=x, target_tensor=y)

# 把 dataset 放入 DataLoader
loader = Data.DataLoader(
    dataset=torch_dataset,      # torch TensorDataset format
    batch_size=BATCH_SIZE,      # mini batch size
    shuffle=True,               # 要不要打乱数据 (打乱比较好)
    num_workers=2,              # 多线程来读数据
)

for epoch in range(3):
    for step, (batch_x, batch_y) in enumerate(loader):
        print('Epoch: ', epoch, '| Step: ', step, '| batch x: ',
              batch_x.numpy(), '| batch y: ', batch_y.numpy())
```

​		之后运行的结果如下所示：

```python
"""
Epoch:0 | Step:0 | batch x:[6. 7. 2. 3. 1.] | batch y:[5. 4. 9. 8. 10.]
Epoch:0 | Step:1 | batch x:[9. 10. 4. 8. 5.] | batch y:[2. 1. 7. 3. 6.]
Epoch:1 | Step:0 | batch x:[3. 4. 2. 9. 10.] | batch y:[8. 7. 9. 2. 1.]
Epoch:1 | Step:1 | batch x:[1. 7. 8. 5. 6.] | batch y:[10. 4. 3. 6. 5.]
Epoch:2 | Step:0 | batch x:[3. 9. 2. 6. 7.] | batch y:[8. 2. 9. 5. 4.]
Epoch:2 | Step:1 | batch x:[10. 4. 8. 1. 5.] | batch y:[1. 7. 3. 10. 6.]
"""
```



## 关于Variable 和 Tensor

​		在学习莫烦的教程中关于优化器的部分时，我注意到了在新版本PyTorch中有关Variable和Tensor之间关系的改变，不过在官方文档中没有发现什么。

​		在教程的视频中对从TensorDataset里获取到的batch数据进行了转换为Variable的操作，解释道是因为只有Variable才能进行自动求导，官方文档也是这么说的。但事实上在教程的代码中取消了这一操作，而在Pycharm的Python控制台中查看变量的类型时，可以清晰地看到所有数据都是Tensor类型的。

```python
# training
for epoch in range(EPOCH):
    print('Epoch: ', epoch)
    for step, (b_x, b_y) in enumerate(loader):
        for net, opt, l_his in zip(nets, optimizers, losses_his):
            output = net(b_x)
            loss = loss_func(output, b_y)
            opt.zero_grad()
            loss.backward()
            opt.step()
            l_his.append(loss.data.numpy())
```

​		询问了同学后得知Tensor变量只需要它的`require_grad`属性是`True`的就可以进行自动求导，经过查证果然如此！上述代码中的b_x和b_y的`require_grad`属性是`False`的，但是loss的`require_grad`属性是`True`的，这应该是在执行`loss_func`时为其加上的属性，这样便能够进行`backward()`操作了！

​		（所以事实证明Python Console是个好东西）
